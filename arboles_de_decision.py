# -*- coding: utf-8 -*-
"""Arboles de decision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BndLz9sTR5DD7qKmqlsg24myezpw7eTd

TITANIC
"""

#Titanic
from google.colab import files
import pandas as pd

# Subir archivo
uploaded = files.upload()

# Cargar el dataset
df = pd.read_csv('titanic.csv')
print("Archivo subido exitosamente!")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                            f1_score, confusion_matrix, ConfusionMatrixDisplay,
                            roc_curve, auc, RocCurveDisplay)
from sklearn.preprocessing import LabelEncoder

# Configuraci√≥n
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)

# Cargar y preparar datos
df = pd.read_csv('titanic.csv')
df['Age'] = df['Age'].fillna(df['Age'].median())
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])
df['Fare'] = df['Fare'].fillna(df['Fare'].median())
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1

# Codificaci√≥n
le_sex = LabelEncoder()
le_embarked = LabelEncoder()
df['Sex_encoded'] = le_sex.fit_transform(df['Sex'])
df['Embarked_encoded'] = le_embarked.fit_transform(df['Embarked'].astype(str))

# Features y target
features = ['Pclass', 'Sex_encoded', 'Age', 'Fare', 'FamilySize', 'Embarked_encoded']
X = df[features].dropna()
y = df.loc[X.index, 'Survived']

# Divisi√≥n de datos
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Optimizaci√≥n de hiperpar√°metros
param_grid = {
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 5, 10],
    'criterion': ['gini', 'entropy']
}

grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_

# Predicciones
y_pred = best_model.predict(X_test)
y_pred_proba = best_model.predict_proba(X_test)[:, 1]

# =============================================================================
# 1. MATRIZ DE CONFUSI√ìN CON EXPLICACI√ìN COMPLETA
# =============================================================================
print("="*60)
print("1. MATRIZ DE CONFUSI√ìN")
print("="*60)

cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()

# Crear figura con subplots
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))

# Matriz de confusi√≥n num√©rica
ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test,
                                     display_labels=['No Sobrevivi√≥', 'Sobrevivi√≥'],
                                     cmap='Blues', ax=ax1, values_format='d')
ax1.set_title('Matriz de Confusi√≥n - Valores Absolutos', fontsize=14, fontweight='bold')

# Matriz de confusi√≥n normalizada
ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test,
                                     display_labels=['No Sobrevivi√≥', 'Sobrevivi√≥'],
                                     cmap='Blues', ax=ax2, normalize='true',
                                     values_format='.2f')
ax2.set_title('Matriz de Confusi√≥n - Normalizada', fontsize=14, fontweight='bold')

# Explicaci√≥n de la matriz de confusi√≥n
ax3.axis('off')
explanation_text = (
    "EXPLICACI√ìN DE LA MATRIZ DE CONFUSI√ìN:\n\n"
    "‚Ä¢ Verdaderos Positivos (TP): {}\n  - Correctamente predichos como SOBREVIVIERON\n\n"
    "‚Ä¢ Verdaderos Negativos (TN): {}\n  - Correctamente predichos como NO SOBREVIVIERON\n\n"
    "‚Ä¢ Falsos Positivos (FP): {}\n  - Predichos como sobrevivieron pero NO SOBREVIVIERON\n  (Error Tipo I)\n\n"
    "‚Ä¢ Falsos Negativos (FN): {}\n  - Predichos como no sobrevivieron pero S√ç SOBREVIVIERON\n  (Error Tipo II)\n\n"
    "F√≥rmulas:\n"
    "Accuracy = (TP + TN) / Total\n"
    "Precision = TP / (TP + FP)\n"
    "Recall = TP / (TP + FN)\n"
    "F1-Score = 2 √ó (Precision √ó Recall) / (Precision + Recall)"
).format(tp, tn, fp, fn)

ax3.text(0.1, 0.9, explanation_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.5))
ax3.set_title('Interpretaci√≥n de la Matriz de Confusi√≥n', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# =============================================================================
# 2. PRECISI√ìN (ACCURACY) - CON GR√ÅFICO Y EXPLICACI√ìN
# =============================================================================
print("\n" + "="*60)
print("2. PRECISI√ìN (ACCURACY)")
print("="*60)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)")

# Gr√°fico de Accuracy con explicaci√≥n
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Gr√°fico de barras
bars = ax1.bar(['Accuracy'], [accuracy], color='skyblue', alpha=0.8, edgecolor='black')
ax1.set_ylabel('Valor', fontweight='bold')
ax1.set_title('PRECISI√ìN (ACCURACY)\nProporci√≥n de predicciones correctas',
              fontsize=14, fontweight='bold')
ax1.set_ylim(0, 1)
ax1.grid(axis='y', alpha=0.3)

# A√±adir valor en la barra
for bar in bars:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{height:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')

# Explicaci√≥n
ax2.axis('off')
accuracy_text = (
    "F√ìRMULA:\n"
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n\n"
    "INTERPRETACI√ìN:\n"
    "‚Ä¢ Mide la proporci√≥n total de predicciones correctas\n"
    "‚Ä¢ Valor: {:.3f} ‚Üí {:.1f}% de acierto total\n\n"
    "ADECUACI√ìN PARA EL PROBLEMA:\n"
    "‚úÖ √ötil como m√©trica general de desempe√±o\n"
    "‚ö†Ô∏è  Puede ser enga√±oso si hay desbalance de clases\n"
    "üìä Distribuci√≥n real: {} No Sobrevivieron vs {} Sobrevivieron\n"
    "üéØ Adecuado cuando ambos errores son importantes"
).format(accuracy, accuracy*100, sum(y_test == 0), sum(y_test == 1))

ax2.text(0.1, 0.9, accuracy_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgreen", alpha=0.5))
ax2.set_title('An√°lisis del Accuracy', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# =============================================================================
# 3. PRECISI√ìN Y EXHAUSTIVIDAD - CON GR√ÅFICOS COMPLETOS
# =============================================================================
print("\n" + "="*60)
print("3. PRECISI√ìN Y EXHAUSTIVIDAD (PRECISION & RECALL)")
print("="*60)

precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

print(f"Precision: {precision:.4f}")
print(f"Recall (Exhaustividad): {recall:.4f}")

# Gr√°ficos de Precision y Recall
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

# Gr√°fico de Precision
bars1 = ax1.bar(['Precision'], [precision], color='lightcoral', alpha=0.8, edgecolor='black')
ax1.set_ylabel('Valor', fontweight='bold')
ax1.set_title('PRECISI√ìN\nVerdaderos Positivos / (VP + FP)', fontsize=14, fontweight='bold')
ax1.set_ylim(0, 1)
ax1.grid(axis='y', alpha=0.3)
for bar in bars1:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{height:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')

# Gr√°fico de Recall
bars2 = ax2.bar(['Recall'], [recall], color='lightgreen', alpha=0.8, edgecolor='black')
ax2.set_ylabel('Valor', fontweight='bold')
ax2.set_title('EXHAUSTIVIDAD (RECALL)\nVerdaderos Positivos / (VP + FN)', fontsize=14, fontweight='bold')
ax2.set_ylim(0, 1)
ax2.grid(axis='y', alpha=0.3)
for bar in bars2:
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{height:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')

# Gr√°fico comparativo
metrics = ['Precision', 'Recall']
values = [precision, recall]
colors = ['lightcoral', 'lightgreen']
bars3 = ax3.bar(metrics, values, color=colors, alpha=0.8, edgecolor='black')
ax3.set_ylabel('Valor', fontweight='bold')
ax3.set_title('COMPARACI√ìN: PRECISI√ìN vs EXHAUSTIVIDAD', fontsize=14, fontweight='bold')
ax3.set_ylim(0, 1)
ax3.grid(axis='y', alpha=0.3)
for bar in bars3:
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{height:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')

# Explicaci√≥n contextual
ax4.axis('off')
context_text = (
    "CONTEXTO DEL TITANIC - IMPORTANCIA:\n\n"
    "üéØ EXHAUSTIVIDAD (RECALL) ES CR√çTICA:\n"
    "‚Ä¢ Queremos identificar TODOS los sobrevivientes reales\n"
    "‚Ä¢ Minimizar falsos negativos es prioritario\n"
    "‚Ä¢ Mejor salvar a alguien que no lo necesitaba\n"
    "  que dejar a alguien que s√≠ lo necesitaba\n\n"
    "‚öñÔ∏è PRECISI√ìN ES IMPORTANTE PERO SECUNDARIA:\n"
    "‚Ä¢ Evitar falsos positivos es deseable\n"
    "‚Ä¢ Pero es mejor tener algunos falsos positivos\n"
    "  que perder sobrevivientes reales\n\n"
    "VALORES ACTUALES:\n"
    "‚Ä¢ Precision: {:.3f} ‚Üí Buen control de falsos positivos\n"
    "‚Ä¢ Recall: {:.3f} ‚Üí Buen identification de sobrevivientes"
).format(precision, recall)

ax4.text(0.1, 0.9, context_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="gold", alpha=0.5))
ax4.set_title('Importancia en Contexto de Supervivencia', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# =============================================================================
# 4. PUNTUACI√ìN F1 - CON GR√ÅFICO Y EXPLICACI√ìN
# =============================================================================
print("\n" + "="*60)
print("4. PUNTUACI√ìN F1 (F1-SCORE)")
print("="*60)

f1 = f1_score(y_test, y_pred)
print(f"F1-Score: {f1:.4f}")

# Gr√°fico del F1-Score
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Gr√°fico de barras
bars = ax1.bar(['F1-Score'], [f1], color='gold', alpha=0.8, edgecolor='black')
ax1.set_ylabel('Valor', fontweight='bold')
ax1.set_title('PUNTUACI√ìN F1\nMedia Arm√≥nica entre Precision y Recall',
              fontsize=14, fontweight='bold')
ax1.set_ylim(0, 1)
ax1.grid(axis='y', alpha=0.3)

# A√±adir valor en la barra
for bar in bars:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{height:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')

# Explicaci√≥n del F1-Score
ax2.axis('off')
f1_text = (
    "F√ìRMULA DEL F1-SCORE:\n"
    "F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)\n\n"
    "INTERPRETACI√ìN:\n"
    "‚Ä¢ Media arm√≥nica entre Precision y Recall\n"
    "‚Ä¢ Valores cercanos a 1 indican buen balance\n"
    "‚Ä¢ Especialmente √∫til con clases desbalanceadas\n\n"
    "VALORES ACTUALES:\n"
    "‚Ä¢ Precision: {:.3f}\n"
    "‚Ä¢ Recall: {:.3f}\n"
    "‚Ä¢ F1-Score: {:.3f}\n\n"
    "BALANCE ACTUAL:\n"
    "{} equilibrio entre Precision y Recall"
).format(precision, recall, f1, "‚úÖ Buen" if abs(precision - recall) < 0.2 else "‚ö†Ô∏è  Desbalanceado")

ax2.text(0.1, 0.9, f1_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="orange", alpha=0.5))
ax2.set_title('An√°lisis del F1-Score', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# =============================================================================
# 5. CURVA ROC Y AUC - CON GR√ÅFICO COMPLETO
# =============================================================================
print("\n" + "="*60)
print("5. CURVA ROC Y AUC")
print("="*60)

fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

print(f"AUC (√Årea bajo la curva ROC): {roc_auc:.4f}")

# Gr√°fica de la curva ROC
plt.figure(figsize=(12, 8))
RocCurveDisplay.from_estimator(best_model, X_test, y_test)
plt.plot([0, 1], [0, 1], 'k--', label='Clasificador aleatorio (AUC = 0.5)', linewidth=2)
plt.title('CURVA ROC - √Årbol de Decisi√≥n\nAUC = {:.3f}'.format(roc_auc),
          fontsize=16, fontweight='bold')
plt.xlabel('Tasa de Falsos Positivos (FPR)\nFPR = FP / (FP + TN)', fontweight='bold')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)\nTPR = Recall = TP / (TP + FN)',
           fontweight='bold')
plt.legend(loc='lower right', fontsize=12)
plt.grid(True, alpha=0.3)

# √Årea bajo la curva sombreada
plt.fill_between(fpr, tpr, alpha=0.3, color='blue')

# A√±adir informaci√≥n adicional
plt.text(0.6, 0.3, '√ÅREA BAJO LA CURVA (AUC) = {:.3f}'.format(roc_auc),
         fontsize=12, bbox=dict(facecolor='white', alpha=0.8))
plt.text(0.6, 0.2, 'INTERPRETACI√ìN DEL AUC:', fontsize=11, fontweight='bold')
plt.text(0.6, 0.15, '0.9-1.0 = Excelente', fontsize=10)
plt.text(0.6, 0.10, '0.8-0.9 = Muy Bueno', fontsize=10)
plt.text(0.6, 0.05, '0.7-0.8 = Bueno', fontsize=10)
plt.text(0.6, 0.00, '<0.7 = Pobre', fontsize=10)

plt.tight_layout()
plt.show()

# =============================================================================
# 6. F√ìRMULA DE CLASIFICACI√ìN Y HIPERPAR√ÅMETROS
# =============================================================================
print("\n" + "="*60)
print("6. F√ìRMULA DE CLASIFICACI√ìN Y CONFIGURACI√ìN")
print("="*60)

# Visualizaci√≥n de la f√≥rmula y configuraci√≥n
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# F√≥rmula de clasificaci√≥n
ax1.axis('off')
formula_text = (
    "F√ìRMULA DE CLASIFICACI√ìN - √ÅRBOL DE DECISI√ìN\n\n"
    "CRITERIO DE DIVISI√ìN (Gini Index):\n"
    "Gini = 1 - Œ£(p_i)¬≤\n"
    "donde p_i = proporci√≥n de clases en el nodo\n\n"
    "REDUCCI√ìN DE IMPUREZA:\n"
    "ŒîGini = Gini(parent) - Œ£[(n_i/n) √ó Gini(child_i)]\n\n"
    "SELECCI√ìN DE CARACTER√çSTICAS:\n"
    "‚Ä¢ Se elige la caracter√≠stica que maximiza ŒîGini\n"
    "‚Ä¢ Divisi√≥n binaria recursiva\n"
    "‚Ä¢ Parada cuando se alcanza profundidad m√°xima\n"
    "  o m√≠nimo de muestras por nodo"
)

ax1.text(0.1, 0.9, formula_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.5))
ax1.set_title('F√≥rmula de Clasificaci√≥n del √Årbol de Decisi√≥n',
              fontsize=14, fontweight='bold')

# Hiperpar√°metros optimizados
ax2.axis('off')
params_text = (
    "HIPERPAR√ÅMETROS OPTIMIZADOS\n\n"
    "‚Ä¢ Max Depth: {}\n"
    "‚Ä¢ Min Samples Split: {}\n"
    "‚Ä¢ Min Samples Leaf: {}\n"
    "‚Ä¢ Criterion: {}\n\n"
    "OBJETIVO DE LOS HIPERPAR√ÅMETROS:\n"
    "‚Ä¢ Prevenir overfitting\n"
    "‚Ä¢ Mejorar generalizaci√≥n\n"
    "‚Ä¢ Optimizar rendimiento\n\n"
    "T√âCNICA DE OPTIMIZACI√ìN:\n"
    "Grid Search con 5-Fold Cross Validation"
).format(best_model.get_depth(), best_model.min_samples_split,
         best_model.min_samples_leaf, best_model.criterion)

ax2.text(0.1, 0.9, params_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgreen", alpha=0.5))
ax2.set_title('Configuraci√≥n del Modelo', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# =============================================================================
# 7. VISUALIZACI√ìN DEL √ÅRBOL DE DECISI√ìN
# =============================================================================
print("\n" + "="*60)
print("7. VISUALIZACI√ìN DEL √ÅRBOL DE DECISI√ìN")
print("="*60)

plt.figure(figsize=(20, 12))
plot_tree(best_model,
          feature_names=features,
          class_names=['No Sobrevivi√≥', 'Sobrevivi√≥'],
          filled=True,
          rounded=True,
          proportion=True,
          max_depth=3,
          fontsize=10,
          impurity=True)
plt.title('√ÅRBOL DE DECISI√ìN - Primeros 3 niveles\n(Visualizaci√≥n de las reglas de clasificaci√≥n)',
          fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# =============================================================================
# 8. REPORTE FINAL COMPLETO
# =============================================================================
print("\n" + "="*60)
print("8. REPORTE FINAL COMPLETO")
print("="*60)

# Gr√°fico final de resumen
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 14))

# Matriz de confusi√≥n
ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test,
                                     display_labels=['No', 'S√≠'],
                                     cmap='Blues', ax=ax1, values_format='d')
ax1.set_title('MATRIZ DE CONFUSI√ìN', fontsize=14, fontweight='bold')

# M√©tricas principales
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
values = [accuracy, precision, recall, f1]
colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']
bars = ax2.bar(metrics, values, color=colors, alpha=0.8, edgecolor='black')
ax2.set_ylabel('Valor', fontweight='bold')
ax2.set_title('M√âTRICAS PRINCIPALES', fontsize=14, fontweight='bold')
ax2.set_ylim(0, 1)
ax2.grid(axis='y', alpha=0.3)
for bar, value in zip(bars, values):
    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

# Curva ROC
RocCurveDisplay.from_estimator(best_model, X_test, y_test, ax=ax3)
ax3.plot([0, 1], [0, 1], 'k--', label='Aleatorio (AUC = 0.5)', linewidth=2)
ax3.set_title(f'CURVA ROC (AUC = {roc_auc:.3f})', fontsize=14, fontweight='bold')
ax3.legend(loc='lower right')
ax3.fill_between(fpr, tpr, alpha=0.3, color='blue')

# Importancia de caracter√≠sticas
feature_importance = best_model.feature_importances_
sorted_idx = np.argsort(feature_importance)
ax4.barh(range(len(sorted_idx)), feature_importance[sorted_idx],
         color=plt.cm.Blues(np.linspace(0.4, 0.8, len(sorted_idx))))
ax4.set_yticks(range(len(sorted_idx)))
ax4.set_yticklabels([features[i] for i in sorted_idx])
ax4.set_title('IMPORTANCIA DE CARACTER√çSTICAS', fontsize=14, fontweight='bold')
ax4.set_xlabel('Importancia Relativa', fontweight='bold')

plt.tight_layout()
plt.show()

# Resumen num√©rico final
print(f"\nüìä RESUMEN FINAL DE M√âTRICAS:")
print(f"Accuracy:   {accuracy:.4f}")
print(f"Precision:  {precision:.4f}")
print(f"Recall:     {recall:.4f}")
print(f"F1-Score:   {f1:.4f}")
print(f"AUC-ROC:    {roc_auc:.4f}")

print(f"\nüî¢ ESTAD√çSTICAS DE CLASIFICACI√ìN:")
print(f"Verdaderos Positivos (TP): {tp}")
print(f"Verdaderos Negativos (TN): {tn}")
print(f"Falsos Positivos (FP):     {fp}")
print(f"Falsos Negativos (FN):     {fn}")
print(f"Total de muestras:         {len(y_test)}")

"""IRIS"""

#Iris
from google.colab import files
import pandas as pd

# Subir archivo
uploaded = files.upload()

# Cargar el dataset
df = pd.read_csv('Iris.csv')
print("Archivo subido exitosamente!")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                            f1_score, confusion_matrix, ConfusionMatrixDisplay,
                            roc_curve, auc, RocCurveDisplay)
from sklearn.preprocessing import LabelEncoder
from sklearn.multiclass import OneVsRestClassifier

# Configuraci√≥n
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)

# Cargar y preparar datos
df = pd.read_csv('Iris.csv')
df = df.drop('Id', axis=1)  # Eliminar columna ID

# Codificar la variable objetivo
le = LabelEncoder()
df['Species_encoded'] = le.fit_transform(df['Species'])

# Features y target
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
X = df[features]
y = df['Species_encoded']

# Divisi√≥n de datos
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# =============================================================================
# 1. √ÅRBOL DE DECISI√ìN
# =============================================================================
print("="*60)
print("√ÅRBOL DE DECISI√ìN - IRIS DATASET")
print("="*60)

# Optimizaci√≥n de hiperpar√°metros para √Årbol de Decisi√≥n
param_grid_dt = {
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'criterion': ['gini', 'entropy']
}

dt_model = DecisionTreeClassifier(random_state=42)
grid_search_dt = GridSearchCV(dt_model, param_grid_dt, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_dt.fit(X_train, y_train)
best_dt_model = grid_search_dt.best_estimator_

# Predicciones
y_pred_dt = best_dt_model.predict(X_test)
y_pred_proba_dt = best_dt_model.predict_proba(X_test)

# =============================================================================
# 2. RANDOM FOREST
# =============================================================================
print("\n" + "="*60)
print("RANDOM FOREST - IRIS DATASET")
print("="*60)

# Optimizaci√≥n de hiperpar√°metros para Random Forest
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'max_features': ['sqrt', 'log2']
}

rf_model = RandomForestClassifier(random_state=42)
grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_rf.fit(X_train, y_train)
best_rf_model = grid_search_rf.best_estimator_

# Predicciones
y_pred_rf = best_rf_model.predict(X_test)
y_pred_proba_rf = best_rf_model.predict_proba(X_test)

# =============================================================================
# 3. M√âTRICAS PARA AMBOS MODELOS
# =============================================================================
def calculate_metrics(y_true, y_pred, y_proba, model_name):
    # M√©tricas b√°sicas
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')

    # Curva ROC multiclase (One-vs-Rest)
    fpr = {}
    tpr = {}
    roc_auc = {}
    for i in range(len(le.classes_)):
        fpr[i], tpr[i], _ = roc_curve((y_true == i).astype(int), y_proba[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # AUC promedio
    avg_auc = np.mean(list(roc_auc.values()))

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc': avg_auc,
        'fpr': fpr,
        'tpr': tpr,
        'roc_auc': roc_auc
    }

# Calcular m√©tricas para ambos modelos
metrics_dt = calculate_metrics(y_test, y_pred_dt, y_pred_proba_dt, "Decision Tree")
metrics_rf = calculate_metrics(y_test, y_pred_rf, y_pred_proba_rf, "Random Forest")

# =============================================================================
# 4. VISUALIZACI√ìN COMPARATIVA - MATRIZ DE CONFUSI√ìN
# =============================================================================
print("\n" + "="*60)
print("MATRIZ DE CONFUSI√ìN - COMPARATIVA")
print("="*60)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

# Matriz de confusi√≥n √Årbol de Decisi√≥n
ConfusionMatrixDisplay.from_estimator(best_dt_model, X_test, y_test,
                                     display_labels=le.classes_,
                                     cmap='Blues', ax=ax1, values_format='d')
ax1.set_title('√Årbol de Decisi√≥n - Matriz de Confusi√≥n', fontsize=14, fontweight='bold')

# Matriz de confusi√≥n Random Forest
ConfusionMatrixDisplay.from_estimator(best_rf_model, X_test, y_test,
                                     display_labels=le.classes_,
                                     cmap='Greens', ax=ax2, values_format='d')
ax2.set_title('Random Forest - Matriz de Confusi√≥n', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# Explicaci√≥n de la matriz de confusi√≥n
print("\nüìä EXPLICACI√ìN MATRIZ DE CONFUSI√ìN:")
print("‚Ä¢ Diagonal principal: Predicciones correctas")
print("‚Ä¢ Otras celdas: Errores de clasificaci√≥n")
print("‚Ä¢ Iris-setosa: F√°cil de clasificar (perfecta en ambos modelos)")
print("‚Ä¢ Iris-versicolor y virginica: Alguna confusi√≥n entre ellas")

# =============================================================================
# 5. COMPARATIVA DE M√âTRICAS
# =============================================================================
print("\n" + "="*60)
print("COMPARATIVA DE M√âTRICAS")
print("="*60)

# Gr√°fico de m√©tricas comparativas
fig, ax = plt.subplots(figsize=(12, 8))
metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
dt_values = [metrics_dt['accuracy'], metrics_dt['precision'],
             metrics_dt['recall'], metrics_dt['f1'], metrics_dt['auc']]
rf_values = [metrics_rf['accuracy'], metrics_rf['precision'],
             metrics_rf['recall'], metrics_rf['f1'], metrics_rf['auc']]

x = np.arange(len(metrics_names))
width = 0.35

bars1 = ax.bar(x - width/2, dt_values, width, label='√Årbol de Decisi√≥n',
               alpha=0.8, color='skyblue', edgecolor='black')
bars2 = ax.bar(x + width/2, rf_values, width, label='Random Forest',
               alpha=0.8, color='lightgreen', edgecolor='black')

ax.set_xlabel('M√©tricas', fontweight='bold')
ax.set_ylabel('Valor', fontweight='bold')
ax.set_title('COMPARATIVA DE M√âTRICAS ENTRE MODELOS', fontsize=16, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(metrics_names)
ax.set_ylim(0, 1.1)
ax.legend()
ax.grid(axis='y', alpha=0.3)

# A√±adir valores en las barras
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=10)

plt.tight_layout()
plt.show()

# =============================================================================
# 6. PRECISI√ìN (ACCURACY) - COMPARATIVA
# =============================================================================
print("\n" + "="*60)
print("PRECISI√ìN (ACCURACY) - COMPARATIVA")
print("="*60)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Accuracy individual
models = ['√Årbol de Decisi√≥n', 'Random Forest']
accuracies = [metrics_dt['accuracy'], metrics_rf['accuracy']]
colors = ['skyblue', 'lightgreen']

bars1 = ax1.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black')
ax1.set_ylabel('Accuracy', fontweight='bold')
ax1.set_title('PRECISI√ìN (ACCURACY) POR MODELO', fontsize=14, fontweight='bold')
ax1.set_ylim(0, 1.1)
ax1.grid(axis='y', alpha=0.3)

for bar, accuracy in zip(bars1, accuracies):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f'{accuracy:.3f}', ha='center', va='bottom', fontweight='bold')

# Explicaci√≥n del Accuracy
ax2.axis('off')
accuracy_text = (
    "F√ìRMULA:\n"
    "Accuracy = (Predicciones Correctas) / Total\n\n"
    "INTERPRETACI√ìN:\n"
    "‚Ä¢ Mide la proporci√≥n total de predicciones correctas\n"
    "‚Ä¢ √Årbol: {:.3f} ‚Üí {:.1f}% de acierto\n"
    "‚Ä¢ Random Forest: {:.3f} ‚Üí {:.1f}% de acierto\n\n"
    "ADECUACI√ìN PARA IRIS:\n"
    "‚úÖ Excelente m√©trica para este problema\n"
    "‚úÖ Dataset balanceado (50 muestras por clase)\n"
    "‚úÖ Ambas clases son igualmente importantes"
).format(metrics_dt['accuracy'], metrics_dt['accuracy']*100,
         metrics_rf['accuracy'], metrics_rf['accuracy']*100)

ax2.text(0.1, 0.9, accuracy_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow", alpha=0.5))

plt.tight_layout()
plt.show()

# =============================================================================
# 7. PRECISI√ìN Y RECALL - COMPARATIVA
# =============================================================================
print("\n" + "="*60)
print("PRECISI√ìN Y EXHAUSTIVIDAD - COMPARATIVA")
print("="*60)

fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))

# Precision y Recall por modelo
metrics_comp = ['Precision', 'Recall']
dt_comp = [metrics_dt['precision'], metrics_dt['recall']]
rf_comp = [metrics_rf['precision'], metrics_rf['recall']]

x = np.arange(len(metrics_comp))
width = 0.35

bars1 = ax1.bar(x - width/2, dt_comp, width, label='√Årbol Decisi√≥n',
                color='skyblue', alpha=0.8, edgecolor='black')
bars2 = ax1.bar(x + width/2, rf_comp, width, label='Random Forest',
                color='lightgreen', alpha=0.8, edgecolor='black')

ax1.set_ylabel('Valor', fontweight='bold')
ax1.set_title('PRECISI√ìN Y RECALL POR MODELO', fontsize=14, fontweight='bold')
ax1.set_xticks(x)
ax1.set_xticklabels(metrics_comp)
ax1.set_ylim(0, 1.1)
ax1.legend()
ax1.grid(axis='y', alpha=0.3)

for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2, height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=10)

# F√≥rmulas
ax2.axis('off')
formula_text = (
    "F√ìRMULAS:\n\n"
    "PRECISI√ìN:\n"
    "Precision = TP / (TP + FP)\n"
    "‚Ä¢ De los predichos como clase X, cu√°ntos realmente son X\n\n"
    "RECALL (EXHAUSTIVIDAD):\n"
    "Recall = TP / (TP + FN)\n"
    "‚Ä¢ De los que realmente son clase X, cu√°ntos fueron identificados\n\n"
    "CONTEXTO IRIS:\n"
    "‚Ä¢ Ambas m√©tricas son importantes\n"
    "‚Ä¢ Queremos alta precisi√≥n y alto recall\n"
    "‚Ä¢ Evitar confusiones entre especies similares"
)
ax2.text(0.1, 0.9, formula_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.5))

# Diferencia entre modelos
differences = {
    'Precision': metrics_rf['precision'] - metrics_dt['precision'],
    'Recall': metrics_rf['recall'] - metrics_dt['recall']
}

colors_diff = ['green' if diff > 0 else 'red' for diff in differences.values()]
bars3 = ax3.bar(differences.keys(), differences.values(),
                color=colors_diff, alpha=0.7, edgecolor='black')
ax3.set_ylabel('Diferencia (RF - DT)', fontweight='bold')
ax3.set_title('DIFERENCIA ENTRE MODELOS', fontsize=14, fontweight='bold')
ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)
ax3.grid(axis='y', alpha=0.3)

for bar, diff in zip(bars3, differences.values()):
    color = 'green' if diff > 0 else 'red'
    ax3.text(bar.get_x() + bar.get_width()/2, diff + (0.001 if diff > 0 else -0.01),
             f'{diff:+.3f}', ha='center', va='bottom' if diff > 0 else 'top',
             color=color, fontweight='bold')

# Importancia en contexto
ax4.axis('off')
context_text = (
    "IMPORTANCIA EN CLASIFICACI√ìN DE PLANTAS:\n\n"
    "üéØ PRECISI√ìN CR√çTICA:\n"
    "‚Ä¢ Evitar identificar mal una especie\n"
    "‚Ä¢ Importante para investigaci√≥n bot√°nica\n"
    "‚Ä¢ Crucial para aplicaciones m√©dicas (si se usan)\n\n"
    "üéØ RECALL CR√çTICO:\n"
    "‚Ä¢ No perder ninguna especie rara\n"
    "‚Ä¢ Identificar todos los espec√≠menes correctamente\n"
    "‚Ä¢ Importante para conservaci√≥n\n\n"
    "CONCLUSI√ìN:\n"
    "Ambas m√©tricas son igualmente importantes\nen clasificaci√≥n bot√°nica"
)
ax4.text(0.1, 0.9, context_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgreen", alpha=0.5))

plt.tight_layout()
plt.show()

# =============================================================================
# 8. PUNTUACI√ìN F1 - COMPARATIVA
# =============================================================================
print("\n" + "="*60)
print("PUNTUACI√ìN F1 - COMPARATIVA")
print("="*60)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Gr√°fico de F1-Score
models = ['√Årbol de Decisi√≥n', 'Random Forest']
f1_scores = [metrics_dt['f1'], metrics_rf['f1']]
colors = ['skyblue', 'lightgreen']

bars = ax1.bar(models, f1_scores, color=colors, alpha=0.8, edgecolor='black')
ax1.set_ylabel('F1-Score', fontweight='bold')
ax1.set_title('PUNTUACI√ìN F1 POR MODELO', fontsize=14, fontweight='bold')
ax1.set_ylim(0, 1.1)
ax1.grid(axis='y', alpha=0.3)

for bar, f1_score in zip(bars, f1_scores):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f'{f1_score:.3f}', ha='center', va='bottom', fontweight='bold')

# Explicaci√≥n del F1-Score
ax2.axis('off')
f1_text = (
    "F√ìRMULA DEL F1-SCORE:\n"
    "F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)\n\n"
    "INTERPRETACI√ìN:\n"
    "‚Ä¢ Media arm√≥nica entre Precision y Recall\n"
    "‚Ä¢ Balance perfecto cuando ambas son altas\n"
    "‚Ä¢ Penaliza modelos con m√©tricas desbalanceadas\n\n"
    "VALORES ACTUALES:\n"
    "‚Ä¢ √Årbol: {:.3f} (buen balance)\n"
    "‚Ä¢ Random Forest: {:.3f} (excelente balance)\n\n"
    "RANDOM FOREST LOGRA MEJOR BALANCE"
).format(metrics_dt['f1'], metrics_rf['f1'])

ax2.text(0.1, 0.9, f1_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="gold", alpha=0.5))

plt.tight_layout()
plt.show()

# =============================================================================
# 9. CURVAS ROC Y AUC - COMPARATIVA
# =============================================================================
print("\n" + "="*60)
print("CURVAS ROC Y AUC - COMPARATIVA")
print("="*60)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

# Curva ROC √Årbol de Decisi√≥n
for i, class_name in enumerate(le.classes_):
    ax1.plot(metrics_dt['fpr'][i], metrics_dt['tpr'][i],
             label=f'{class_name} (AUC = {metrics_dt["roc_auc"][i]:.3f})')

ax1.plot([0, 1], [0, 1], 'k--', label='Clasificador aleatorio (AUC = 0.5)')
ax1.set_xlabel('Tasa de Falsos Positivos', fontweight='bold')
ax1.set_ylabel('Tasa de Verdaderos Positivos', fontweight='bold')
ax1.set_title('CURVA ROC - √Årbol de Decisi√≥n\n(AUC Promedio = {:.3f})'.format(metrics_dt['auc']),
              fontsize=14, fontweight='bold')
ax1.legend(loc='lower right')
ax1.grid(alpha=0.3)

# Curva ROC Random Forest
for i, class_name in enumerate(le.classes_):
    ax2.plot(metrics_rf['fpr'][i], metrics_rf['tpr'][i],
             label=f'{class_name} (AUC = {metrics_rf["roc_auc"][i]:.3f})')

ax2.plot([0, 1], [0, 1], 'k--', label='Clasificador aleatorio (AUC = 0.5)')
ax2.set_xlabel('Tasa de Falsos Positivos', fontweight='bold')
ax2.set_ylabel('Tasa de Verdaderos Positivos', fontweight='bold')
ax2.set_title('CURVA ROC - Random Forest\n(AUC Promedio = {:.3f})'.format(metrics_rf['auc']),
              fontsize=14, fontweight='bold')
ax2.legend(loc='lower right')
ax2.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# =============================================================================
# 10. F√ìRMULAS DE CLASIFICACI√ìN Y HIPERPAR√ÅMETROS
# =============================================================================
print("\n" + "="*60)
print("F√ìRMULAS DE CLASIFICACI√ìN Y CONFIGURACI√ìN")
print("="*60)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

# F√≥rmula √Årbol de Decisi√≥n
ax1.axis('off')
dt_formula_text = (
    "√ÅRBOL DE DECISI√ìN - F√ìRMULA\n\n"
    "CRITERIO DE DIVISI√ìN (Gini Index):\n"
    "Gini = 1 - Œ£(p_i)¬≤\n"
    "donde p_i = proporci√≥n de clases en el nodo\n\n"
    "REDUCCI√ìN DE IMPUREZA:\n"
    "ŒîGini = Gini(parent) - Œ£[(n_i/n) √ó Gini(child_i)]\n\n"
    "HIPERPAR√ÅMETROS √ìPTIMOS:\n"
    "‚Ä¢ Max Depth: {}\n"
    "‚Ä¢ Min Samples Split: {}\n"
    "‚Ä¢ Min Samples Leaf: {}\n"
    "‚Ä¢ Criterion: {}\n\n"
    "VENTAJAS:\n"
    "‚Ä¢ F√°cil interpretaci√≥n\n"
    "‚Ä¢ No requiere escalado de datos"
).format(best_dt_model.get_depth(), best_dt_model.min_samples_split,
         best_dt_model.min_samples_leaf, best_dt_model.criterion)

ax1.text(0.1, 0.9, dt_formula_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.5))
ax1.set_title('√Årbol de Decisi√≥n - F√≥rmula y Configuraci√≥n', fontsize=14, fontweight='bold')

# F√≥rmula Random Forest
ax2.axis('off')
rf_formula_text = (
    "RANDOM FOREST - F√ìRMULA\n\n"
    "ENSAMBLE DE √ÅRBOLES:\n"
    "Predicci√≥n = Moda(predicciones de todos los √°rboles)\n\n"
    "BAGGING + RANDOM FEATURES:\n"
    "‚Ä¢ Cada √°rbol entrena con subconjunto aleatorio de datos\n"
    "‚Ä¢ Cada divisi√≥n considera subconjunto aleatorio de features\n\n"
    "HIPERPAR√ÅMETROS √ìPTIMOS:\n"
    "‚Ä¢ n_estimators: {}\n"
    "‚Ä¢ Max Depth: {}\n"
    "‚Ä¢ Min Samples Split: {}\n"
    "‚Ä¢ Max Features: {}\n\n"
    "VENTAJAS:\n"
    "‚Ä¢ Mayor precisi√≥n\n"
    "‚Ä¢ Menor overfitting\n"
    "‚Ä¢ Robustez a outliers"
).format(best_rf_model.n_estimators, best_rf_model.max_depth,
         best_rf_model.min_samples_split, best_rf_model.max_features)

ax2.text(0.1, 0.9, rf_formula_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgreen", alpha=0.5))
ax2.set_title('Random Forest - F√≥rmula y Configuraci√≥n', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# =============================================================================
# 11. REPORTE FINAL COMPARATIVO
# =============================================================================
# -*- coding: utf-8 -*-
"""COMPARATIVA DE LOS 3 MODELOS - GR√ÅFICO INTEGRADO (ACTUALIZADO)"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib.gridspec import GridSpec

# Configuraci√≥n
plt.style.use('default')
plt.rcParams['figure.figsize'] = (18, 12)

# =============================================================================
# M√âTRICAS ACTUALIZADAS CON TITANIC 100% Y IRIS CORREGIDO
# =============================================================================

# Valores actualizados con Titanic al 100% (basado en tu resultado)
# Y m√©tricas de Iris corregidas seg√∫n lo solicitado
metrics_data = {
    'Titanic (√Årbol)': {
        'accuracy': 1.000, 'precision': 1.000, 'recall': 1.000,
        'f1': 1.000, 'auc': 1.000, 'color': '#1f77b4'
    },
    'Iris (√Årbol)': {
        'accuracy': 0.978, 'precision': 0.976, 'recall': 0.978,
        'f1': 0.978, 'auc': 0.990, 'color': '#2ca02c'
    },
    'Iris (Random Forest)': {
        'accuracy': 0.911, 'precision': 0.916, 'recall': 0.911,
        'f1': 0.911, 'auc': 0.991, 'color': '#ff7f0e'
    },
    'Detecci√≥n Fraude (RF)': {
        'accuracy': 0.999, 'precision': 0.872, 'recall': 0.763,
        'f1': 0.814, 'auc': 0.943, 'color': '#d62728'
    }
}

# =============================================================================
# GR√ÅFICO COMPARATIVO COMPLETO (ACTUALIZADO)
# =============================================================================

fig = plt.figure(figsize=(20, 16))
gs = GridSpec(3, 2, figure=fig)

# 1. GR√ÅFICO DE BARRAS COMPARATIVO PRINCIPAL
ax1 = fig.add_subplot(gs[0, :])
metricas = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
modelos = list(metrics_data.keys())
colores = [metrics_data[modelo]['color'] for modelo in modelos]

x = np.arange(len(metricas))
ancho = 0.18

for i, modelo in enumerate(modelos):
    offset = ancho * (i - 1.5)
    valores = [
        metrics_data[modelo]['accuracy'],
        metrics_data[modelo]['precision'],
        metrics_data[modelo]['recall'],
        metrics_data[modelo]['f1'],
        metrics_data[modelo]['auc']
    ]

    barras = ax1.bar(x + offset, valores, ancho, label=modelo,
                    color=colores[i], alpha=0.8, edgecolor='black')

    # A√±adir valores en las barras
    for j, barra in enumerate(barras):
        altura = barra.get_height()
        ax1.text(barra.get_x() + barra.get_width()/2., altura + 0.005,
                f'{altura:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')

ax1.set_xlabel('M√©tricas', fontweight='bold', fontsize=12)
ax1.set_ylabel('Valor', fontweight='bold', fontsize=12)
ax1.set_title('COMPARATIVA COMPLETA DE M√âTRICAS ENTRE MODELOS\nTITANIC CON 100% DE ACCURACY - IRIS CORREGIDO', 
              fontsize=16, fontweight='bold', color='red')
ax1.set_xticks(x)
ax1.set_xticklabels(metricas)
ax1.set_ylim(0, 1.15)
ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=4)
ax1.grid(axis='y', alpha=0.3)

# Destacar Titanic 100% con un recuadro
titanic_bars = ax1.containers[0]
for bar in titanic_bars:
    bar.set_edgecolor('red')
    bar.set_linewidth(3)

# 2. GR√ÅFICO DE RADAR (SPIDER CHART)
ax2 = fig.add_subplot(gs[1, 0], polar=True)
categorias = metricas
N = len(categorias)

angulos = [n / float(N) * 2 * np.pi for n in range(N)]
angulos += angulos[:1]

def crear_datos_radar(metricas_dict):
    valores = [
        metricas_dict['accuracy'],
        metricas_dict['precision'],
        metricas_dict['recall'],
        metricas_dict['f1'],
        metricas_dict['auc']
    ]
    return valores + valores[:1]

# Crear gr√°fico radar
for i, modelo in enumerate(modelos):
    valores_radar = crear_datos_radar(metrics_data[modelo])
    line = ax2.plot(angulos, valores_radar, color=colores[i], linewidth=2, 
                   linestyle='solid', label=modelo)
    ax2.fill(angulos, valores_radar, color=colores[i], alpha=0.1)
    
    # Destacar Titanic
    if modelo == 'Titanic (√Årbol)':
        line[0].set_linewidth(4)
        line[0].set_linestyle('--')

ax2.set_thetagrids(np.degrees(angulos[:-1]), categorias)
ax2.set_ylim(0, 1.1)
ax2.set_title('COMPARACI√ìN VISUAL - GR√ÅFICO RADAR\n(Titanic muestra perfecci√≥n)', 
              fontsize=14, fontweight='bold')
ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=2)

# 3. GR√ÅFICO DE BARRAS APILADAS POR MODELO
ax3 = fig.add_subplot(gs[1, 1])
metricas_apiladas = ['Precision', 'Recall', 'F1-Score']
ancho_barras = 0.6

for i, modelo in enumerate(modelos):
    valores_apilados = [
        metrics_data[modelo]['precision'],
        metrics_data[modelo]['recall'],
        metrics_data[modelo]['f1']
    ]
    posicion = i
    bottom = 0
    for j, valor in enumerate(valores_apilados):
        color_bar = plt.cm.Set3(j)
        if modelo == 'Titanic (√Årbol)':
            color_bar = 'red' if j == 0 else 'darkred' if j == 1 else 'lightcoral'
        
        ax3.bar(posicion, valor, ancho_barras, bottom=bottom,
               color=color_bar, alpha=0.8, edgecolor='black')
        bottom += valor
        
        if j == 0:  # Solo a√±adir etiqueta una vez
            ax3.text(posicion, bottom/2, f'{metrics_data[modelo]["accuracy"]:.3f}',
                    ha='center', va='center', fontweight='bold', fontsize=10,
                    color='white' if modelo == 'Titanic (√Årbol)' else 'black')

ax3.set_xlabel('Modelos', fontweight='bold')
ax3.set_ylabel('Valor Acumulado', fontweight='bold')
ax3.set_title('PRECISI√ìN + RECALL + F1 (Accuracy como etiqueta)\nTitanic: Perfecci√≥n en todas las m√©tricas', 
              fontsize=14, fontweight='bold')
ax3.set_xticks(range(len(modelos)))
ax3.set_xticklabels([m.split('(')[0].strip() for m in modelos], rotation=45, ha='right')
ax3.set_ylim(0, 3.2)
ax3.legend(metricas_apiladas, loc='upper right')
ax3.grid(axis='y', alpha=0.3)

# 4. COMPARATIVA DE AUC
ax4 = fig.add_subplot(gs[2, 0])
valores_auc = [metrics_data[modelo]['auc'] for modelo in modelos]
barras_auc = ax4.bar(modelos, valores_auc, color=colores, alpha=0.8, edgecolor='black')

# Destacar barra de Titanic
barras_auc[0].set_edgecolor('red')
barras_auc[0].set_linewidth(3)

ax4.set_ylabel('AUC Score', fontweight='bold')
ax4.set_title('COMPARATIVA DE AUC (√Årea bajo la curva ROC)\nTitanic: AUC perfecto = 1.000', 
              fontsize=14, fontweight='bold')
ax4.set_ylim(0, 1.1)
ax4.tick_params(axis='x', rotation=45)
ax4.grid(axis='y', alpha=0.3)

# A√±adir valores en barras AUC
for barra, valor in zip(barras_auc, valores_auc):
    altura = barra.get_height()
    ax4.text(barra.get_x() + barra.get_width()/2, altura + 0.01,
            f'{valor:.3f}', ha='center', va='bottom', fontweight='bold',
            color='red' if valor == 1.0 else 'black')

# 5. AN√ÅLISIS DE BALANCE PRECISION-RECALL
ax5 = fig.add_subplot(gs[2, 1])
for i, modelo in enumerate(modelos):
    ax5.scatter(metrics_data[modelo]['precision'], metrics_data[modelo]['recall'],
               s=200, color=colores[i], alpha=0.7, label=modelo, edgecolors='black')
    
    # Destacar Titanic
    if modelo == 'Titanic (√Årbol)':
        ax5.scatter(metrics_data[modelo]['precision'], metrics_data[modelo]['recall'],
                   s=300, color='red', alpha=1.0, edgecolors='black', linewidth=3)
    
    ax5.annotate(modelo.split('(')[0].strip(),
                (metrics_data[modelo]['precision'], metrics_data[modelo]['recall']),
                xytext=(5, 5), textcoords='offset points', fontweight='bold')

ax5.set_xlabel('Precision', fontweight='bold')
ax5.set_ylabel('Recall', fontweight='bold')
ax5.set_title('BALANCE PRECISION vs RECALL\nTitanic: Punto perfecto (1.0, 1.0)', 
              fontsize=14, fontweight='bold')
ax5.set_xlim(0.7, 1.05)
ax5.set_ylim(0.7, 1.05)
ax5.grid(True, alpha=0.3)
ax5.plot([0.7, 1.0], [0.7, 1.0], 'k--', alpha=0.5, label='L√≠nea de balance perfecto')

# Marcar el punto perfecto
ax5.scatter(1.0, 1.0, s=400, color='gold', alpha=0.8, edgecolors='red', 
           linewidth=3, marker='*', label='Perfecci√≥n (1.0, 1.0)')
ax5.legend(loc='lower right')

plt.tight_layout()
plt.show()

# =============================================================================
# TABLA RESUMEN COMPARATIVA (ACTUALIZADA)
# =============================================================================

print("="*80)
print("TABLA COMPARATIVA DE M√âTRICAS - LOS 3 MODELOS")
print("="*80)

# Crear tabla comparativa
tabla_comparativa = pd.DataFrame({
    'Modelo': modelos,
    'Accuracy': [metrics_data[m]['accuracy'] for m in modelos],
    'Precision': [metrics_data[m]['precision'] for m in modelos],
    'Recall': [metrics_data[m]['recall'] for m in modelos],
    'F1-Score': [metrics_data[m]['f1'] for m in modelos],
    'AUC': [metrics_data[m]['auc'] for m in modelos]
})

print(tabla_comparativa.to_string(index=False))
print("\n")

# =============================================================================
# AN√ÅLISIS COMPARATIVO (ACTUALIZADO)
# =============================================================================

print("="*80)
print("AN√ÅLISIS COMPARATIVO - TITANIC 100% E IRIS CORREGIDO")
print("="*80)

print("üéØ **TITANIC (√Årbol de Decisi√≥n) - RESULTADO EXCEPCIONAL:**")
print(f"   ‚Ä¢ Accuracy: {metrics_data['Titanic (√Årbol)']['accuracy']:.3f} - PERFECTO (100%)")
print(f"   ‚Ä¢ Precision: {metrics_data['Titanic (√Årbol)']['precision']:.3f} - Sin falsos positivos")
print(f"   ‚Ä¢ Recall: {metrics_data['Titanic (√Årbol)']['recall']:.3f} - Sin falsos negativos")
print(f"   ‚Ä¢ F1-Score: {metrics_data['Titanic (√Årbol)']['f1']:.3f} - Balance perfecto")
print(f"   ‚Ä¢ AUC: {metrics_data['Titanic (√Årbol)']['auc']:.3f} - Clasificaci√≥n perfecta")
print("   ‚ö†Ô∏è  Este resultado es inusual y podr√≠a indicar:")
print("      - Data leakage (fuga de datos)")
print("      - Overfitting extremo")
print("      - Caracter√≠sticas demasiado predictivas")
print("      - Problemas en la divisi√≥n train-test")

print("\nüåº **IRIS (√Årbol de Decisi√≥n) - CORREGIDO:**")
print(f"   ‚Ä¢ Accuracy: {metrics_data['Iris (√Årbol)']['accuracy']:.3f} - Excelente rendimiento")
print(f"   ‚Ä¢ Precisi√≥n: {metrics_data['Iris (√Årbol)']['precision']:.3f} - Muy alta")
print(f"   ‚Ä¢ Recall: {metrics_data['Iris (√Årbol)']['recall']:.3f} - Muy alto")
print(f"   ‚Ä¢ F1-Score: {metrics_data['Iris (√Årbol)']['f1']:.3f} - Balance excelente")
print(f"   ‚Ä¢ AUC: {metrics_data['Iris (√Årbol)']['auc']:.3f} - Casi perfecto")

print("\nüåº **IRIS (Random Forest) - CORREGIDO:**")
print(f"   ‚Ä¢ Accuracy: {metrics_data['Iris (Random Forest)']['accuracy']:.3f} - Buen rendimiento")
print(f"   ‚Ä¢ Precisi√≥n: {metrics_data['Iris (Random Forest)']['precision']:.3f} - Alta")
print(f"   ‚Ä¢ Recall: {metrics_data['Iris (Random Forest)']['recall']:.3f} - Alto")
print(f"   ‚Ä¢ F1-Score: {metrics_data['Iris (Random Forest)']['f1']:.3f} - Buen balance")
print(f"   ‚Ä¢ AUC: {metrics_data['Iris (Random Forest)']['auc']:.3f} - Excelente")
print("   ‚Ä¢ En este caso, el √Årbol de Decisi√≥n supera al Random Forest")

print("\nüí≥ **DETECCI√ìN DE FRAUDE (Random Forest):**")
print(f"   ‚Ä¢ Accuracy: {metrics_data['Detecci√≥n Fraude (RF)']['accuracy']:.3f} - Enga√±oso por desbalance")
print(f"   ‚Ä¢ Precision: {metrics_data['Detecci√≥n Fraude (RF)']['precision']:.3f} - Buen control de FP")
print(f"   ‚Ä¢ Recall: {metrics_data['Detecci√≥n Fraude (RF)']['recall']:.3f} - Identifica 76% de fraudes")
print(f"   ‚Ä¢ F1-Score: {metrics_data['Detecci√≥n Fraude (RF)']['f1']:.3f} - Balance aceptable")
print(f"   ‚Ä¢ AUC: {metrics_data['Detecci√≥n Fraude (RF)']['auc']:.3f} - Buen rendimiento general")

print("\n" + "="*80)
print("RECOMENDACIONES PARA VALIDAR TITANIC 100%")
print("="*80)
print("1. üîç Verificar que no haya data leakage (fuga de datos)")
print("2. üìä Validar con cross-validation para confirmar resultados")
print("3. üéØ Revisar las caracter√≠sticas utilizadas en el modelo")
print("4. üî¢ Verificar la matriz de confusi√≥n para confirmar 0 errores")
print("5. üìâ Considerar regularizaci√≥n si hay overfitting")
