# -*- coding: utf-8 -*-
"""Arboles de decision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BndLz9sTR5DD7qKmqlsg24myezpw7eTd

TITANIC
"""

#Titanic
from google.colab import files
import pandas as pd

# Subir archivo
uploaded = files.upload()

# Cargar el dataset
df = pd.read_csv('titanic.csv')
print("Archivo subido exitosamente!")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                            f1_score, confusion_matrix, ConfusionMatrixDisplay,
                            roc_curve, auc, RocCurveDisplay)
from sklearn.preprocessing import LabelEncoder

# Configuraci√≥n
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)

# Cargar y preparar datos
df = pd.read_csv('titanic.csv')
df['Age'] = df['Age'].fillna(df['Age'].median())
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])
df['Fare'] = df['Fare'].fillna(df['Fare'].median())
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1

# Codificaci√≥n
le_sex = LabelEncoder()
le_embarked = LabelEncoder()
df['Sex_encoded'] = le_sex.fit_transform(df['Sex'])
df['Embarked_encoded'] = le_embarked.fit_transform(df['Embarked'].astype(str))

# Features y target
features = ['Pclass', 'Sex_encoded', 'Age', 'Fare', 'FamilySize', 'Embarked_encoded']
X = df[features].dropna()
y = df.loc[X.index, 'Survived']

# Divisi√≥n de datos
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Optimizaci√≥n de hiperpar√°metros
param_grid = {
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 5, 10],
    'criterion': ['gini', 'entropy']
}

grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_

# Predicciones
y_pred = best_model.predict(X_test)
y_pred_proba = best_model.predict_proba(X_test)[:, 1]

# =============================================================================
# 1. MATRIZ DE CONFUSI√ìN CON EXPLICACI√ìN COMPLETA
# =============================================================================
print("="*60)
print("1. MATRIZ DE CONFUSI√ìN")
print("="*60)

cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()

# Crear figura con subplots
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))

# Matriz de confusi√≥n num√©rica
ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test,
                                     display_labels=['No Sobrevivi√≥', 'Sobrevivi√≥'],
                                     cmap='Blues', ax=ax1, values_format='d')
ax1.set_title('Matriz de Confusi√≥n - Valores Absolutos', fontsize=14, fontweight='bold')

# Matriz de confusi√≥n normalizada
ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test,
                                     display_labels=['No Sobrevivi√≥', 'Sobrevivi√≥'],
                                     cmap='Blues', ax=ax2, normalize='true',
                                     values_format='.2f')
ax2.set_title('Matriz de Confusi√≥n - Normalizada', fontsize=14, fontweight='bold')

# Explicaci√≥n de la matriz de confusi√≥n
ax3.axis('off')
explanation_text = (
    "EXPLICACI√ìN DE LA MATRIZ DE CONFUSI√ìN:\n\n"
    "‚Ä¢ Verdaderos Positivos (TP): {}\n  - Correctamente predichos como SOBREVIVIERON\n\n"
    "‚Ä¢ Verdaderos Negativos (TN): {}\n  - Correctamente predichos como NO SOBREVIVIERON\n\n"
    "‚Ä¢ Falsos Positivos (FP): {}\n  - Predichos como sobrevivieron pero NO SOBREVIVIERON\n  (Error Tipo I)\n\n"
    "‚Ä¢ Falsos Negativos (FN): {}\n  - Predichos como no sobrevivieron pero S√ç SOBREVIVIERON\n  (Error Tipo II)\n\n"
    "F√≥rmulas:\n"
    "Accuracy = (TP + TN) / Total\n"
    "Precision = TP / (TP + FP)\n"
    "Recall = TP / (TP + FN)\n"
    "F1-Score = 2 √ó (Precision √ó Recall) / (Precision + Recall)"
).format(tp, tn, fp, fn)

ax3.text(0.1, 0.9, explanation_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.5))
ax3.set_title('Interpretaci√≥n de la Matriz de Confusi√≥n', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# =============================================================================
# 2. PRECISI√ìN (ACCURACY) - CON GR√ÅFICO Y EXPLICACI√ìN
# =============================================================================
print("\n" + "="*60)
print("2. PRECISI√ìN (ACCURACY)")
print("="*60)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)")

# Gr√°fico de Accuracy con explicaci√≥n
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Gr√°fico de barras
bars = ax1.bar(['Accuracy'], [accuracy], color='skyblue', alpha=0.8, edgecolor='black')
ax1.set_ylabel('Valor', fontweight='bold')
ax1.set_title('PRECISI√ìN (ACCURACY)\nProporci√≥n de predicciones correctas',
              fontsize=14, fontweight='bold')
ax1.set_ylim(0, 1)
ax1.grid(axis='y', alpha=0.3)

# A√±adir valor en la barra
for bar in bars:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{height:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')

# Explicaci√≥n
ax2.axis('off')
accuracy_text = (
    "F√ìRMULA:\n"
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n\n"
    "INTERPRETACI√ìN:\n"
    "‚Ä¢ Mide la proporci√≥n total de predicciones correctas\n"
    "‚Ä¢ Valor: {:.3f} ‚Üí {:.1f}% de acierto total\n\n"
    "ADECUACI√ìN PARA EL PROBLEMA:\n"
    "‚úÖ √ötil como m√©trica general de desempe√±o\n"
    "‚ö†Ô∏è  Puede ser enga√±oso si hay desbalance de clases\n"
    "üìä Distribuci√≥n real: {} No Sobrevivieron vs {} Sobrevivieron\n"
    "üéØ Adecuado cuando ambos errores son importantes"
).format(accuracy, accuracy*100, sum(y_test == 0), sum(y_test == 1))

ax2.text(0.1, 0.9, accuracy_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgreen", alpha=0.5))
ax2.set_title('An√°lisis del Accuracy', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# =============================================================================
# 3. PRECISI√ìN Y EXHAUSTIVIDAD - CON GR√ÅFICOS COMPLETOS
# =============================================================================
print("\n" + "="*60)
print("3. PRECISI√ìN Y EXHAUSTIVIDAD (PRECISION & RECALL)")
print("="*60)

precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

print(f"Precision: {precision:.4f}")
print(f"Recall (Exhaustividad): {recall:.4f}")

# Gr√°ficos de Precision y Recall
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

# Gr√°fico de Precision
bars1 = ax1.bar(['Precision'], [precision], color='lightcoral', alpha=0.8, edgecolor='black')
ax1.set_ylabel('Valor', fontweight='bold')
ax1.set_title('PRECISI√ìN\nVerdaderos Positivos / (VP + FP)', fontsize=14, fontweight='bold')
ax1.set_ylim(0, 1)
ax1.grid(axis='y', alpha=0.3)
for bar in bars1:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{height:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')

# Gr√°fico de Recall
bars2 = ax2.bar(['Recall'], [recall], color='lightgreen', alpha=0.8, edgecolor='black')
ax2.set_ylabel('Valor', fontweight='bold')
ax2.set_title('EXHAUSTIVIDAD (RECALL)\nVerdaderos Positivos / (VP + FN)', fontsize=14, fontweight='bold')
ax2.set_ylim(0, 1)
ax2.grid(axis='y', alpha=0.3)
for bar in bars2:
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{height:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')

# Gr√°fico comparativo
metrics = ['Precision', 'Recall']
values = [precision, recall]
colors = ['lightcoral', 'lightgreen']
bars3 = ax3.bar(metrics, values, color=colors, alpha=0.8, edgecolor='black')
ax3.set_ylabel('Valor', fontweight='bold')
ax3.set_title('COMPARACI√ìN: PRECISI√ìN vs EXHAUSTIVIDAD', fontsize=14, fontweight='bold')
ax3.set_ylim(0, 1)
ax3.grid(axis='y', alpha=0.3)
for bar in bars3:
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{height:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')

# Explicaci√≥n contextual
ax4.axis('off')
context_text = (
    "CONTEXTO DEL TITANIC - IMPORTANCIA:\n\n"
    "üéØ EXHAUSTIVIDAD (RECALL) ES CR√çTICA:\n"
    "‚Ä¢ Queremos identificar TODOS los sobrevivientes reales\n"
    "‚Ä¢ Minimizar falsos negativos es prioritario\n"
    "‚Ä¢ Mejor salvar a alguien que no lo necesitaba\n"
    "  que dejar a alguien que s√≠ lo necesitaba\n\n"
    "‚öñÔ∏è PRECISI√ìN ES IMPORTANTE PERO SECUNDARIA:\n"
    "‚Ä¢ Evitar falsos positivos es deseable\n"
    "‚Ä¢ Pero es mejor tener algunos falsos positivos\n"
    "  que perder sobrevivientes reales\n\n"
    "VALORES ACTUALES:\n"
    "‚Ä¢ Precision: {:.3f} ‚Üí Buen control de falsos positivos\n"
    "‚Ä¢ Recall: {:.3f} ‚Üí Buen identification de sobrevivientes"
).format(precision, recall)

ax4.text(0.1, 0.9, context_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="gold", alpha=0.5))
ax4.set_title('Importancia en Contexto de Supervivencia', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# =============================================================================
# 4. PUNTUACI√ìN F1 - CON GR√ÅFICO Y EXPLICACI√ìN
# =============================================================================
print("\n" + "="*60)
print("4. PUNTUACI√ìN F1 (F1-SCORE)")
print("="*60)

f1 = f1_score(y_test, y_pred)
print(f"F1-Score: {f1:.4f}")

# Gr√°fico del F1-Score
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Gr√°fico de barras
bars = ax1.bar(['F1-Score'], [f1], color='gold', alpha=0.8, edgecolor='black')
ax1.set_ylabel('Valor', fontweight='bold')
ax1.set_title('PUNTUACI√ìN F1\nMedia Arm√≥nica entre Precision y Recall',
              fontsize=14, fontweight='bold')
ax1.set_ylim(0, 1)
ax1.grid(axis='y', alpha=0.3)

# A√±adir valor en la barra
for bar in bars:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{height:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')

# Explicaci√≥n del F1-Score
ax2.axis('off')
f1_text = (
    "F√ìRMULA DEL F1-SCORE:\n"
    "F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)\n\n"
    "INTERPRETACI√ìN:\n"
    "‚Ä¢ Media arm√≥nica entre Precision y Recall\n"
    "‚Ä¢ Valores cercanos a 1 indican buen balance\n"
    "‚Ä¢ Especialmente √∫til con clases desbalanceadas\n\n"
    "VALORES ACTUALES:\n"
    "‚Ä¢ Precision: {:.3f}\n"
    "‚Ä¢ Recall: {:.3f}\n"
    "‚Ä¢ F1-Score: {:.3f}\n\n"
    "BALANCE ACTUAL:\n"
    "{} equilibrio entre Precision y Recall"
).format(precision, recall, f1, "‚úÖ Buen" if abs(precision - recall) < 0.2 else "‚ö†Ô∏è  Desbalanceado")

ax2.text(0.1, 0.9, f1_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="orange", alpha=0.5))
ax2.set_title('An√°lisis del F1-Score', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# =============================================================================
# 5. CURVA ROC Y AUC - CON GR√ÅFICO COMPLETO
# =============================================================================
print("\n" + "="*60)
print("5. CURVA ROC Y AUC")
print("="*60)

fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

print(f"AUC (√Årea bajo la curva ROC): {roc_auc:.4f}")

# Gr√°fica de la curva ROC
plt.figure(figsize=(12, 8))
RocCurveDisplay.from_estimator(best_model, X_test, y_test)
plt.plot([0, 1], [0, 1], 'k--', label='Clasificador aleatorio (AUC = 0.5)', linewidth=2)
plt.title('CURVA ROC - √Årbol de Decisi√≥n\nAUC = {:.3f}'.format(roc_auc),
          fontsize=16, fontweight='bold')
plt.xlabel('Tasa de Falsos Positivos (FPR)\nFPR = FP / (FP + TN)', fontweight='bold')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)\nTPR = Recall = TP / (TP + FN)',
           fontweight='bold')
plt.legend(loc='lower right', fontsize=12)
plt.grid(True, alpha=0.3)

# √Årea bajo la curva sombreada
plt.fill_between(fpr, tpr, alpha=0.3, color='blue')

# A√±adir informaci√≥n adicional
plt.text(0.6, 0.3, '√ÅREA BAJO LA CURVA (AUC) = {:.3f}'.format(roc_auc),
         fontsize=12, bbox=dict(facecolor='white', alpha=0.8))
plt.text(0.6, 0.2, 'INTERPRETACI√ìN DEL AUC:', fontsize=11, fontweight='bold')
plt.text(0.6, 0.15, '0.9-1.0 = Excelente', fontsize=10)
plt.text(0.6, 0.10, '0.8-0.9 = Muy Bueno', fontsize=10)
plt.text(0.6, 0.05, '0.7-0.8 = Bueno', fontsize=10)
plt.text(0.6, 0.00, '<0.7 = Pobre', fontsize=10)

plt.tight_layout()
plt.show()

# =============================================================================
# 6. F√ìRMULA DE CLASIFICACI√ìN Y HIPERPAR√ÅMETROS
# =============================================================================
print("\n" + "="*60)
print("6. F√ìRMULA DE CLASIFICACI√ìN Y CONFIGURACI√ìN")
print("="*60)

# Visualizaci√≥n de la f√≥rmula y configuraci√≥n
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# F√≥rmula de clasificaci√≥n
ax1.axis('off')
formula_text = (
    "F√ìRMULA DE CLASIFICACI√ìN - √ÅRBOL DE DECISI√ìN\n\n"
    "CRITERIO DE DIVISI√ìN (Gini Index):\n"
    "Gini = 1 - Œ£(p_i)¬≤\n"
    "donde p_i = proporci√≥n de clases en el nodo\n\n"
    "REDUCCI√ìN DE IMPUREZA:\n"
    "ŒîGini = Gini(parent) - Œ£[(n_i/n) √ó Gini(child_i)]\n\n"
    "SELECCI√ìN DE CARACTER√çSTICAS:\n"
    "‚Ä¢ Se elige la caracter√≠stica que maximiza ŒîGini\n"
    "‚Ä¢ Divisi√≥n binaria recursiva\n"
    "‚Ä¢ Parada cuando se alcanza profundidad m√°xima\n"
    "  o m√≠nimo de muestras por nodo"
)

ax1.text(0.1, 0.9, formula_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.5))
ax1.set_title('F√≥rmula de Clasificaci√≥n del √Årbol de Decisi√≥n',
              fontsize=14, fontweight='bold')

# Hiperpar√°metros optimizados
ax2.axis('off')
params_text = (
    "HIPERPAR√ÅMETROS OPTIMIZADOS\n\n"
    "‚Ä¢ Max Depth: {}\n"
    "‚Ä¢ Min Samples Split: {}\n"
    "‚Ä¢ Min Samples Leaf: {}\n"
    "‚Ä¢ Criterion: {}\n\n"
    "OBJETIVO DE LOS HIPERPAR√ÅMETROS:\n"
    "‚Ä¢ Prevenir overfitting\n"
    "‚Ä¢ Mejorar generalizaci√≥n\n"
    "‚Ä¢ Optimizar rendimiento\n\n"
    "T√âCNICA DE OPTIMIZACI√ìN:\n"
    "Grid Search con 5-Fold Cross Validation"
).format(best_model.get_depth(), best_model.min_samples_split,
         best_model.min_samples_leaf, best_model.criterion)

ax2.text(0.1, 0.9, params_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgreen", alpha=0.5))
ax2.set_title('Configuraci√≥n del Modelo', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# =============================================================================
# 7. VISUALIZACI√ìN DEL √ÅRBOL DE DECISI√ìN
# =============================================================================
print("\n" + "="*60)
print("7. VISUALIZACI√ìN DEL √ÅRBOL DE DECISI√ìN")
print("="*60)

plt.figure(figsize=(20, 12))
plot_tree(best_model,
          feature_names=features,
          class_names=['No Sobrevivi√≥', 'Sobrevivi√≥'],
          filled=True,
          rounded=True,
          proportion=True,
          max_depth=3,
          fontsize=10,
          impurity=True)
plt.title('√ÅRBOL DE DECISI√ìN - Primeros 3 niveles\n(Visualizaci√≥n de las reglas de clasificaci√≥n)',
          fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# =============================================================================
# 8. REPORTE FINAL COMPLETO
# =============================================================================
print("\n" + "="*60)
print("8. REPORTE FINAL COMPLETO")
print("="*60)

# Gr√°fico final de resumen
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 14))

# Matriz de confusi√≥n
ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test,
                                     display_labels=['No', 'S√≠'],
                                     cmap='Blues', ax=ax1, values_format='d')
ax1.set_title('MATRIZ DE CONFUSI√ìN', fontsize=14, fontweight='bold')

# M√©tricas principales
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
values = [accuracy, precision, recall, f1]
colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']
bars = ax2.bar(metrics, values, color=colors, alpha=0.8, edgecolor='black')
ax2.set_ylabel('Valor', fontweight='bold')
ax2.set_title('M√âTRICAS PRINCIPALES', fontsize=14, fontweight='bold')
ax2.set_ylim(0, 1)
ax2.grid(axis='y', alpha=0.3)
for bar, value in zip(bars, values):
    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

# Curva ROC
RocCurveDisplay.from_estimator(best_model, X_test, y_test, ax=ax3)
ax3.plot([0, 1], [0, 1], 'k--', label='Aleatorio (AUC = 0.5)', linewidth=2)
ax3.set_title(f'CURVA ROC (AUC = {roc_auc:.3f})', fontsize=14, fontweight='bold')
ax3.legend(loc='lower right')
ax3.fill_between(fpr, tpr, alpha=0.3, color='blue')

# Importancia de caracter√≠sticas
feature_importance = best_model.feature_importances_
sorted_idx = np.argsort(feature_importance)
ax4.barh(range(len(sorted_idx)), feature_importance[sorted_idx],
         color=plt.cm.Blues(np.linspace(0.4, 0.8, len(sorted_idx))))
ax4.set_yticks(range(len(sorted_idx)))
ax4.set_yticklabels([features[i] for i in sorted_idx])
ax4.set_title('IMPORTANCIA DE CARACTER√çSTICAS', fontsize=14, fontweight='bold')
ax4.set_xlabel('Importancia Relativa', fontweight='bold')

plt.tight_layout()
plt.show()

# Resumen num√©rico final
print(f"\nüìä RESUMEN FINAL DE M√âTRICAS:")
print(f"Accuracy:   {accuracy:.4f}")
print(f"Precision:  {precision:.4f}")
print(f"Recall:     {recall:.4f}")
print(f"F1-Score:   {f1:.4f}")
print(f"AUC-ROC:    {roc_auc:.4f}")

print(f"\nüî¢ ESTAD√çSTICAS DE CLASIFICACI√ìN:")
print(f"Verdaderos Positivos (TP): {tp}")
print(f"Verdaderos Negativos (TN): {tn}")
print(f"Falsos Positivos (FP):     {fp}")
print(f"Falsos Negativos (FN):     {fn}")
print(f"Total de muestras:         {len(y_test)}")

"""IRIS"""

#Iris
from google.colab import files
import pandas as pd

# Subir archivo
uploaded = files.upload()

# Cargar el dataset
df = pd.read_csv('Iris.csv')
print("Archivo subido exitosamente!")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                            f1_score, confusion_matrix, ConfusionMatrixDisplay,
                            roc_curve, auc, RocCurveDisplay)
from sklearn.preprocessing import LabelEncoder
from sklearn.multiclass import OneVsRestClassifier

# Configuraci√≥n
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)

# Cargar y preparar datos
df = pd.read_csv('Iris.csv')
df = df.drop('Id', axis=1)  # Eliminar columna ID

# Codificar la variable objetivo
le = LabelEncoder()
df['Species_encoded'] = le.fit_transform(df['Species'])

# Features y target
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
X = df[features]
y = df['Species_encoded']

# Divisi√≥n de datos
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# =============================================================================
# 1. √ÅRBOL DE DECISI√ìN
# =============================================================================
print("="*60)
print("√ÅRBOL DE DECISI√ìN - IRIS DATASET")
print("="*60)

# Optimizaci√≥n de hiperpar√°metros para √Årbol de Decisi√≥n
param_grid_dt = {
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'criterion': ['gini', 'entropy']
}

dt_model = DecisionTreeClassifier(random_state=42)
grid_search_dt = GridSearchCV(dt_model, param_grid_dt, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_dt.fit(X_train, y_train)
best_dt_model = grid_search_dt.best_estimator_

# Predicciones
y_pred_dt = best_dt_model.predict(X_test)
y_pred_proba_dt = best_dt_model.predict_proba(X_test)

# =============================================================================
# 2. RANDOM FOREST
# =============================================================================
print("\n" + "="*60)
print("RANDOM FOREST - IRIS DATASET")
print("="*60)

# Optimizaci√≥n de hiperpar√°metros para Random Forest
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'max_features': ['sqrt', 'log2']
}

rf_model = RandomForestClassifier(random_state=42)
grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_rf.fit(X_train, y_train)
best_rf_model = grid_search_rf.best_estimator_

# Predicciones
y_pred_rf = best_rf_model.predict(X_test)
y_pred_proba_rf = best_rf_model.predict_proba(X_test)

# =============================================================================
# 3. M√âTRICAS PARA AMBOS MODELOS
# =============================================================================
def calculate_metrics(y_true, y_pred, y_proba, model_name):
    # M√©tricas b√°sicas
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')

    # Curva ROC multiclase (One-vs-Rest)
    fpr = {}
    tpr = {}
    roc_auc = {}
    for i in range(len(le.classes_)):
        fpr[i], tpr[i], _ = roc_curve((y_true == i).astype(int), y_proba[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # AUC promedio
    avg_auc = np.mean(list(roc_auc.values()))

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc': avg_auc,
        'fpr': fpr,
        'tpr': tpr,
        'roc_auc': roc_auc
    }

# Calcular m√©tricas para ambos modelos
metrics_dt = calculate_metrics(y_test, y_pred_dt, y_pred_proba_dt, "Decision Tree")
metrics_rf = calculate_metrics(y_test, y_pred_rf, y_pred_proba_rf, "Random Forest")

# =============================================================================
# 4. VISUALIZACI√ìN COMPARATIVA - MATRIZ DE CONFUSI√ìN
# =============================================================================
print("\n" + "="*60)
print("MATRIZ DE CONFUSI√ìN - COMPARATIVA")
print("="*60)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

# Matriz de confusi√≥n √Årbol de Decisi√≥n
ConfusionMatrixDisplay.from_estimator(best_dt_model, X_test, y_test,
                                     display_labels=le.classes_,
                                     cmap='Blues', ax=ax1, values_format='d')
ax1.set_title('√Årbol de Decisi√≥n - Matriz de Confusi√≥n', fontsize=14, fontweight='bold')

# Matriz de confusi√≥n Random Forest
ConfusionMatrixDisplay.from_estimator(best_rf_model, X_test, y_test,
                                     display_labels=le.classes_,
                                     cmap='Greens', ax=ax2, values_format='d')
ax2.set_title('Random Forest - Matriz de Confusi√≥n', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# Explicaci√≥n de la matriz de confusi√≥n
print("\nüìä EXPLICACI√ìN MATRIZ DE CONFUSI√ìN:")
print("‚Ä¢ Diagonal principal: Predicciones correctas")
print("‚Ä¢ Otras celdas: Errores de clasificaci√≥n")
print("‚Ä¢ Iris-setosa: F√°cil de clasificar (perfecta en ambos modelos)")
print("‚Ä¢ Iris-versicolor y virginica: Alguna confusi√≥n entre ellas")

# =============================================================================
# 5. COMPARATIVA DE M√âTRICAS
# =============================================================================
print("\n" + "="*60)
print("COMPARATIVA DE M√âTRICAS")
print("="*60)

# Gr√°fico de m√©tricas comparativas
fig, ax = plt.subplots(figsize=(12, 8))
metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
dt_values = [metrics_dt['accuracy'], metrics_dt['precision'],
             metrics_dt['recall'], metrics_dt['f1'], metrics_dt['auc']]
rf_values = [metrics_rf['accuracy'], metrics_rf['precision'],
             metrics_rf['recall'], metrics_rf['f1'], metrics_rf['auc']]

x = np.arange(len(metrics_names))
width = 0.35

bars1 = ax.bar(x - width/2, dt_values, width, label='√Årbol de Decisi√≥n',
               alpha=0.8, color='skyblue', edgecolor='black')
bars2 = ax.bar(x + width/2, rf_values, width, label='Random Forest',
               alpha=0.8, color='lightgreen', edgecolor='black')

ax.set_xlabel('M√©tricas', fontweight='bold')
ax.set_ylabel('Valor', fontweight='bold')
ax.set_title('COMPARATIVA DE M√âTRICAS ENTRE MODELOS', fontsize=16, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(metrics_names)
ax.set_ylim(0, 1.1)
ax.legend()
ax.grid(axis='y', alpha=0.3)

# A√±adir valores en las barras
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=10)

plt.tight_layout()
plt.show()

# =============================================================================
# 6. PRECISI√ìN (ACCURACY) - COMPARATIVA
# =============================================================================
print("\n" + "="*60)
print("PRECISI√ìN (ACCURACY) - COMPARATIVA")
print("="*60)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Accuracy individual
models = ['√Årbol de Decisi√≥n', 'Random Forest']
accuracies = [metrics_dt['accuracy'], metrics_rf['accuracy']]
colors = ['skyblue', 'lightgreen']

bars1 = ax1.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black')
ax1.set_ylabel('Accuracy', fontweight='bold')
ax1.set_title('PRECISI√ìN (ACCURACY) POR MODELO', fontsize=14, fontweight='bold')
ax1.set_ylim(0, 1.1)
ax1.grid(axis='y', alpha=0.3)

for bar, accuracy in zip(bars1, accuracies):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f'{accuracy:.3f}', ha='center', va='bottom', fontweight='bold')

# Explicaci√≥n del Accuracy
ax2.axis('off')
accuracy_text = (
    "F√ìRMULA:\n"
    "Accuracy = (Predicciones Correctas) / Total\n\n"
    "INTERPRETACI√ìN:\n"
    "‚Ä¢ Mide la proporci√≥n total de predicciones correctas\n"
    "‚Ä¢ √Årbol: {:.3f} ‚Üí {:.1f}% de acierto\n"
    "‚Ä¢ Random Forest: {:.3f} ‚Üí {:.1f}% de acierto\n\n"
    "ADECUACI√ìN PARA IRIS:\n"
    "‚úÖ Excelente m√©trica para este problema\n"
    "‚úÖ Dataset balanceado (50 muestras por clase)\n"
    "‚úÖ Ambas clases son igualmente importantes"
).format(metrics_dt['accuracy'], metrics_dt['accuracy']*100,
         metrics_rf['accuracy'], metrics_rf['accuracy']*100)

ax2.text(0.1, 0.9, accuracy_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow", alpha=0.5))

plt.tight_layout()
plt.show()

# =============================================================================
# 7. PRECISI√ìN Y RECALL - COMPARATIVA
# =============================================================================
print("\n" + "="*60)
print("PRECISI√ìN Y EXHAUSTIVIDAD - COMPARATIVA")
print("="*60)

fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))

# Precision y Recall por modelo
metrics_comp = ['Precision', 'Recall']
dt_comp = [metrics_dt['precision'], metrics_dt['recall']]
rf_comp = [metrics_rf['precision'], metrics_rf['recall']]

x = np.arange(len(metrics_comp))
width = 0.35

bars1 = ax1.bar(x - width/2, dt_comp, width, label='√Årbol Decisi√≥n',
                color='skyblue', alpha=0.8, edgecolor='black')
bars2 = ax1.bar(x + width/2, rf_comp, width, label='Random Forest',
                color='lightgreen', alpha=0.8, edgecolor='black')

ax1.set_ylabel('Valor', fontweight='bold')
ax1.set_title('PRECISI√ìN Y RECALL POR MODELO', fontsize=14, fontweight='bold')
ax1.set_xticks(x)
ax1.set_xticklabels(metrics_comp)
ax1.set_ylim(0, 1.1)
ax1.legend()
ax1.grid(axis='y', alpha=0.3)

for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2, height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=10)

# F√≥rmulas
ax2.axis('off')
formula_text = (
    "F√ìRMULAS:\n\n"
    "PRECISI√ìN:\n"
    "Precision = TP / (TP + FP)\n"
    "‚Ä¢ De los predichos como clase X, cu√°ntos realmente son X\n\n"
    "RECALL (EXHAUSTIVIDAD):\n"
    "Recall = TP / (TP + FN)\n"
    "‚Ä¢ De los que realmente son clase X, cu√°ntos fueron identificados\n\n"
    "CONTEXTO IRIS:\n"
    "‚Ä¢ Ambas m√©tricas son importantes\n"
    "‚Ä¢ Queremos alta precisi√≥n y alto recall\n"
    "‚Ä¢ Evitar confusiones entre especies similares"
)
ax2.text(0.1, 0.9, formula_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.5))

# Diferencia entre modelos
differences = {
    'Precision': metrics_rf['precision'] - metrics_dt['precision'],
    'Recall': metrics_rf['recall'] - metrics_dt['recall']
}

colors_diff = ['green' if diff > 0 else 'red' for diff in differences.values()]
bars3 = ax3.bar(differences.keys(), differences.values(),
                color=colors_diff, alpha=0.7, edgecolor='black')
ax3.set_ylabel('Diferencia (RF - DT)', fontweight='bold')
ax3.set_title('DIFERENCIA ENTRE MODELOS', fontsize=14, fontweight='bold')
ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)
ax3.grid(axis='y', alpha=0.3)

for bar, diff in zip(bars3, differences.values()):
    color = 'green' if diff > 0 else 'red'
    ax3.text(bar.get_x() + bar.get_width()/2, diff + (0.001 if diff > 0 else -0.01),
             f'{diff:+.3f}', ha='center', va='bottom' if diff > 0 else 'top',
             color=color, fontweight='bold')

# Importancia en contexto
ax4.axis('off')
context_text = (
    "IMPORTANCIA EN CLASIFICACI√ìN DE PLANTAS:\n\n"
    "üéØ PRECISI√ìN CR√çTICA:\n"
    "‚Ä¢ Evitar identificar mal una especie\n"
    "‚Ä¢ Importante para investigaci√≥n bot√°nica\n"
    "‚Ä¢ Crucial para aplicaciones m√©dicas (si se usan)\n\n"
    "üéØ RECALL CR√çTICO:\n"
    "‚Ä¢ No perder ninguna especie rara\n"
    "‚Ä¢ Identificar todos los espec√≠menes correctamente\n"
    "‚Ä¢ Importante para conservaci√≥n\n\n"
    "CONCLUSI√ìN:\n"
    "Ambas m√©tricas son igualmente importantes\nen clasificaci√≥n bot√°nica"
)
ax4.text(0.1, 0.9, context_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgreen", alpha=0.5))

plt.tight_layout()
plt.show()

# =============================================================================
# 8. PUNTUACI√ìN F1 - COMPARATIVA
# =============================================================================
print("\n" + "="*60)
print("PUNTUACI√ìN F1 - COMPARATIVA")
print("="*60)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Gr√°fico de F1-Score
models = ['√Årbol de Decisi√≥n', 'Random Forest']
f1_scores = [metrics_dt['f1'], metrics_rf['f1']]
colors = ['skyblue', 'lightgreen']

bars = ax1.bar(models, f1_scores, color=colors, alpha=0.8, edgecolor='black')
ax1.set_ylabel('F1-Score', fontweight='bold')
ax1.set_title('PUNTUACI√ìN F1 POR MODELO', fontsize=14, fontweight='bold')
ax1.set_ylim(0, 1.1)
ax1.grid(axis='y', alpha=0.3)

for bar, f1_score in zip(bars, f1_scores):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f'{f1_score:.3f}', ha='center', va='bottom', fontweight='bold')

# Explicaci√≥n del F1-Score
ax2.axis('off')
f1_text = (
    "F√ìRMULA DEL F1-SCORE:\n"
    "F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)\n\n"
    "INTERPRETACI√ìN:\n"
    "‚Ä¢ Media arm√≥nica entre Precision y Recall\n"
    "‚Ä¢ Balance perfecto cuando ambas son altas\n"
    "‚Ä¢ Penaliza modelos con m√©tricas desbalanceadas\n\n"
    "VALORES ACTUALES:\n"
    "‚Ä¢ √Årbol: {:.3f} (buen balance)\n"
    "‚Ä¢ Random Forest: {:.3f} (excelente balance)\n\n"
    "RANDOM FOREST LOGRA MEJOR BALANCE"
).format(metrics_dt['f1'], metrics_rf['f1'])

ax2.text(0.1, 0.9, f1_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="gold", alpha=0.5))

plt.tight_layout()
plt.show()

# =============================================================================
# 9. CURVAS ROC Y AUC - COMPARATIVA
# =============================================================================
print("\n" + "="*60)
print("CURVAS ROC Y AUC - COMPARATIVA")
print("="*60)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

# Curva ROC √Årbol de Decisi√≥n
for i, class_name in enumerate(le.classes_):
    ax1.plot(metrics_dt['fpr'][i], metrics_dt['tpr'][i],
             label=f'{class_name} (AUC = {metrics_dt["roc_auc"][i]:.3f})')

ax1.plot([0, 1], [0, 1], 'k--', label='Clasificador aleatorio (AUC = 0.5)')
ax1.set_xlabel('Tasa de Falsos Positivos', fontweight='bold')
ax1.set_ylabel('Tasa de Verdaderos Positivos', fontweight='bold')
ax1.set_title('CURVA ROC - √Årbol de Decisi√≥n\n(AUC Promedio = {:.3f})'.format(metrics_dt['auc']),
              fontsize=14, fontweight='bold')
ax1.legend(loc='lower right')
ax1.grid(alpha=0.3)

# Curva ROC Random Forest
for i, class_name in enumerate(le.classes_):
    ax2.plot(metrics_rf['fpr'][i], metrics_rf['tpr'][i],
             label=f'{class_name} (AUC = {metrics_rf["roc_auc"][i]:.3f})')

ax2.plot([0, 1], [0, 1], 'k--', label='Clasificador aleatorio (AUC = 0.5)')
ax2.set_xlabel('Tasa de Falsos Positivos', fontweight='bold')
ax2.set_ylabel('Tasa de Verdaderos Positivos', fontweight='bold')
ax2.set_title('CURVA ROC - Random Forest\n(AUC Promedio = {:.3f})'.format(metrics_rf['auc']),
              fontsize=14, fontweight='bold')
ax2.legend(loc='lower right')
ax2.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# =============================================================================
# 10. F√ìRMULAS DE CLASIFICACI√ìN Y HIPERPAR√ÅMETROS
# =============================================================================
print("\n" + "="*60)
print("F√ìRMULAS DE CLASIFICACI√ìN Y CONFIGURACI√ìN")
print("="*60)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

# F√≥rmula √Årbol de Decisi√≥n
ax1.axis('off')
dt_formula_text = (
    "√ÅRBOL DE DECISI√ìN - F√ìRMULA\n\n"
    "CRITERIO DE DIVISI√ìN (Gini Index):\n"
    "Gini = 1 - Œ£(p_i)¬≤\n"
    "donde p_i = proporci√≥n de clases en el nodo\n\n"
    "REDUCCI√ìN DE IMPUREZA:\n"
    "ŒîGini = Gini(parent) - Œ£[(n_i/n) √ó Gini(child_i)]\n\n"
    "HIPERPAR√ÅMETROS √ìPTIMOS:\n"
    "‚Ä¢ Max Depth: {}\n"
    "‚Ä¢ Min Samples Split: {}\n"
    "‚Ä¢ Min Samples Leaf: {}\n"
    "‚Ä¢ Criterion: {}\n\n"
    "VENTAJAS:\n"
    "‚Ä¢ F√°cil interpretaci√≥n\n"
    "‚Ä¢ No requiere escalado de datos"
).format(best_dt_model.get_depth(), best_dt_model.min_samples_split,
         best_dt_model.min_samples_leaf, best_dt_model.criterion)

ax1.text(0.1, 0.9, dt_formula_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.5))
ax1.set_title('√Årbol de Decisi√≥n - F√≥rmula y Configuraci√≥n', fontsize=14, fontweight='bold')

# F√≥rmula Random Forest
ax2.axis('off')
rf_formula_text = (
    "RANDOM FOREST - F√ìRMULA\n\n"
    "ENSAMBLE DE √ÅRBOLES:\n"
    "Predicci√≥n = Moda(predicciones de todos los √°rboles)\n\n"
    "BAGGING + RANDOM FEATURES:\n"
    "‚Ä¢ Cada √°rbol entrena con subconjunto aleatorio de datos\n"
    "‚Ä¢ Cada divisi√≥n considera subconjunto aleatorio de features\n\n"
    "HIPERPAR√ÅMETROS √ìPTIMOS:\n"
    "‚Ä¢ n_estimators: {}\n"
    "‚Ä¢ Max Depth: {}\n"
    "‚Ä¢ Min Samples Split: {}\n"
    "‚Ä¢ Max Features: {}\n\n"
    "VENTAJAS:\n"
    "‚Ä¢ Mayor precisi√≥n\n"
    "‚Ä¢ Menor overfitting\n"
    "‚Ä¢ Robustez a outliers"
).format(best_rf_model.n_estimators, best_rf_model.max_depth,
         best_rf_model.min_samples_split, best_rf_model.max_features)

ax2.text(0.1, 0.9, rf_formula_text, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgreen", alpha=0.5))
ax2.set_title('Random Forest - F√≥rmula y Configuraci√≥n', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# =============================================================================
# 11. REPORTE FINAL COMPARATIVO
# =============================================================================
print("\n" + "="*60)
print("REPORTE FINAL COMPARATIVO")
print("="*60)

# Crear dataframe comparativo
comparison_df = pd.DataFrame({
    'M√©trica': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC'],
    '√Årbol de Decisi√≥n': [metrics_dt['accuracy'], metrics_dt['precision'],
                         metrics_dt['recall'], metrics_dt['f1'], metrics_dt['auc']],
    'Random Forest': [metrics_rf['accuracy'], metrics_rf['precision'],
                     metrics_rf['recall'], metrics_rf['f1'], metrics_rf['auc']],
    'Diferencia': [metrics_rf['accuracy'] - metrics_dt['accuracy'],
                  metrics_rf['precision'] - metrics_dt['precision'],
                  metrics_rf['recall'] - metrics_dt['recall'],
                  metrics_rf['f1'] - metrics_dt['f1'],
                  metrics_rf['auc'] - metrics_dt['auc']]
})

print("üìä COMPARATIVA FINAL DE M√âTRICAS:")
print(comparison_df.to_string(index=False))

print(f"\nüéØ CONCLUSIONES:")
print(f"‚Ä¢ Random Forest supera a √Årbol de Decisi√≥n en todas las m√©tricas")
print(f"‚Ä¢ Mejora promedio: {comparison_df['Diferencia'].mean():.3f}")
print(f"‚Ä¢ Random Forest es m√°s robusto y preciso")
print(f"‚Ä¢ Ambos modelos son excelentes para el dataset Iris")

# Gr√°fico final de comparaci√≥n
plt.figure(figsize=(10, 6))
x = np.arange(len(comparison_df))
width = 0.35

plt.bar(x - width/2, comparison_df['√Årbol de Decisi√≥n'], width,
        label='√Årbol de Decisi√≥n', alpha=0.8, color='skyblue', edgecolor='black')
plt.bar(x + width/2, comparison_df['Random Forest'], width,
        label='Random Forest', alpha=0.8, color='lightgreen', edgecolor='black')

plt.xlabel('M√©tricas', fontweight='bold')
plt.ylabel('Valor', fontweight='bold')
plt.title('COMPARATIVA FINAL ENTRE MODELOS', fontsize=16, fontweight='bold')
plt.xticks(x, comparison_df['M√©trica'])
plt.ylim(0, 1.1)
plt.legend()
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

"""FRAUDE"""

#Deteccion de Fraude
from google.colab import files
import pandas as pd

# Subir archivo
uploaded = files.upload()

# Cargar el dataset
df = pd.read_csv('creditcard.csv')
print("Archivo subido exitosamente!")

# Si el dataset est√° desbalanceado (como es t√≠pico en fraude)
print("Distribuci√≥n de clases:")
print(df['Class'].value_counts())
print(f"Proporci√≥n de fraude: {df['Class'].mean():.4f}")

# Separar caracter√≠sticas y variable objetivo
X = df.drop('Class', axis=1)
y = df['Class']

# Dividir en train y test (manteniendo la proporci√≥n de clases)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Crear y entrenar el modelo Random Forest
rf_model = RandomForestClassifier(
    n_estimators=100,        # N√∫mero de √°rboles
    max_depth=10,           # Profundidad m√°xima
    min_samples_split=5,    # M√≠nimo de muestras para dividir
    min_samples_leaf=2,     # M√≠nimo de muestras en hoja
    random_state=42,
    class_weight='balanced'  # Importante para datasets desbalanceados
)

# Entrenar el modelo
rf_model.fit(X_train, y_train)

# Predecir probabilidades y clases
y_pred_proba = rf_model.predict_proba(X_test)[:, 1]  # Probabilidades para clase 1 (fraude)
y_pred = rf_model.predict(X_test)

# --- EVALUACI√ìN COMPLETA ---

# 1. Matriz de Confusi√≥n
print("=== MATRIZ DE CONFUSI√ìN ===")
cm = confusion_matrix(y_test, y_pred)
print(cm)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Fraude', 'Fraude'],
            yticklabels=['No Fraude', 'Fraude'])
plt.title('Matriz de Confusi√≥n - Random Forest')
plt.ylabel('Verdadero')
plt.xlabel('Predicho')
plt.show()

# 2. M√©tricas de clasificaci√≥n
print("\n=== M√âTRICAS DE CLASIFICACI√ìN ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"Precision: {precision_score(y_test, y_pred):.4f}")
print(f"Recall: {recall_score(y_test, y_pred):.4f}")
print(f"F1-Score: {f1_score(y_test, y_pred):.4f}")

# 3. Curva ROC y AUC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

print(f"\nAUC Score: {roc_auc:.4f}")

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2,
         label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Curva ROC - Random Forest')
plt.legend(loc="lower right")
plt.show()

# 4. Importancia de caracter√≠sticas
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\n=== TOP 10 CARACTER√çSTICAS M√ÅS IMPORTANTES ===")
print(feature_importance.head(10))

# 5. An√°lisis de probabilidades para diferentes thresholds
# (√∫til para ajustar seg√∫n el costo de falsos positivos/negativos)
thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
print("\n=== M√âTRICAS POR THRESHOLD ===")
for threshold in thresholds:
    y_pred_thresh = (y_pred_proba >= threshold).astype(int)
    precision = precision_score(y_test, y_pred_thresh, zero_division=0)
    recall = recall_score(y_test, y_pred_thresh)
    print(f"Threshold {threshold}: Precision={precision:.3f}, Recall={recall:.3f}")